{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Simpsons Image Classification."
      ],
      "metadata": {
        "id": "3qIQxv5KPi_H"
      },
      "id": "3qIQxv5KPi_H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial Variables"
      ],
      "metadata": {
        "id": "OP1YHhGGoN6e"
      },
      "id": "OP1YHhGGoN6e"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "\n",
        "# The dataset to download from Kaggle\n",
        "KAGGLE_DATASET = \"alexattia/the-simpsons-characters-dataset\"\n",
        "\n",
        "# The directory where the dataset is extracted\n",
        "KAGGLE_UNZIP_DATASET_DIR = \"/content/simpsons_data\"\n",
        "\n",
        "# The Google Drive Folder ID to read / write files\n",
        "GOOGLE_DRIVE_FOLDER_ID = \"1GZ0NBMKvCcNAvPdW50j6OwcSasaoK8A1\"\n",
        "\n",
        "# Source Dataset Directory\n",
        "SOURCE_DATASET = f\"{KAGGLE_UNZIP_DATASET_DIR}/simpsons_dataset\"\n",
        "\n",
        "# The pre processed dataset with labels\n",
        "PRE_PROCESSED_DATASET = \"simpsons_data_processed.npz\"\n",
        "\n",
        "# The Image Size (width,height)\n",
        "IMG_WIDTH = 64\n",
        "IMG_HEIGHT = 64\n",
        "\n",
        "# Request permissions to access (read/write) the Google Drive Folder ID\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "print(f\"Successful initialization: Dataset: {KAGGLE_DATASET} - Google Drive Id: {GOOGLE_DRIVE_FOLDER_ID}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIDmj-MdobE4",
        "outputId": "14e41be0-05ef-4cfd-9c2e-5a40016e4435"
      },
      "id": "AIDmj-MdobE4",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful initialization: Dataset: alexattia/the-simpsons-characters-dataset - Google Drive Id: 1GZ0NBMKvCcNAvPdW50j6OwcSasaoK8A1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Download Dataset."
      ],
      "metadata": {
        "id": "i3szLDo3S5Ox"
      },
      "id": "i3szLDo3S5Ox"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Getting Kaggle credentials and setting in environment\n",
        "os.environ['KAGGLE_USERNAME'] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ['KAGGLE_KEY'] = userdata.get('KAGGLE_KEY')\n",
        "\n",
        "# Download and unzip dataset from Kaggle\n",
        "!kaggle datasets download -d {KAGGLE_DATASET}\n",
        "!unzip -q the-simpsons-characters-dataset.zip -d {KAGGLE_UNZIP_DATASET_DIR}\n",
        "\n",
        "print(f\"Downloaded {KAGGLE_DATASET} in {KAGGLE_UNZIP_DATASET_DIR}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gYgY2cKQEZq",
        "outputId": "738bdf04-e313-447b-f85d-d316c9cf3c7d"
      },
      "id": "-gYgY2cKQEZq",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset\n",
            "License(s): CC-BY-NC-SA-4.0\n",
            "Downloading the-simpsons-characters-dataset.zip to /content\n",
            "100% 1.07G/1.08G [00:12<00:00, 127MB/s]\n",
            "100% 1.08G/1.08G [00:12<00:00, 93.8MB/s]\n",
            "replace /content/simpsons_data/annotation.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "Downloaded alexattia/the-simpsons-characters-dataset in /content/simpsons_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Prepare Dataset."
      ],
      "metadata": {
        "id": "_H0lwmIxTTHq"
      },
      "id": "_H0lwmIxTTHq"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def load_simpsons_dataset_with_labels(directory, img_width, img_height):\n",
        "    print(\"Loading datasets with labels may take a while\")\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(directory))\n",
        "    class_map = {name: i for i, name in enumerate(class_names)}\n",
        "\n",
        "    # Browse folders\n",
        "    for class_name in class_names:\n",
        "        class_dir = os.path.join(directory, class_name)\n",
        "        if not os.path.isdir(class_dir):\n",
        "            continue\n",
        "\n",
        "        class_idx = class_map[class_name]\n",
        "\n",
        "        # Read images from each folder\n",
        "        for img_name in os.listdir(class_dir):\n",
        "            img_path = os.path.join(class_dir, img_name)\n",
        "            try:\n",
        "                # Read image with OpenCV\n",
        "                img = cv2.imread(img_path)\n",
        "                if img is not None:\n",
        "                    # Convert BGR (OpenCV) to RGB\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                    # Resize (CNN needs fixed size)\n",
        "                    img = cv2.resize(img, (img_width, img_height))\n",
        "\n",
        "                    images.append(img)\n",
        "                    labels.append(class_idx)\n",
        "            except Exception as e:\n",
        "                print(f\"Error cargando {img_path}: {e}\")\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    X = np.array(images)\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # Mix data (Shuffle)\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    return X, y, class_names\n",
        "\n",
        "\n",
        "def save_full_dataset(X_full, y_full, class_names):\n",
        "  np.savez_compressed(PRE_PROCESSED_DATASET, X=X_full, y=y_full, labels=class_names)\n",
        "\n",
        "  file_metadata = {\n",
        "      'name': 'simpsons_data_processed.npz',\n",
        "      'parents': [GOOGLE_DRIVE_FOLDER_ID]\n",
        "  }\n",
        "  media = MediaFileUpload(PRE_PROCESSED_DATASET, mimetype='application/octet-stream')\n",
        "\n",
        "  file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "  print(f\"Dataset successfully stored. ID in Drive: {file.get('id')}\")\n",
        "\n",
        "\n",
        "X_full, y_full, class_names = load_simpsons_dataset_with_labels(SOURCE_DATASET, IMG_WIDTH, IMG_HEIGHT)\n",
        "save_full_dataset(X_full, y_full, class_names)\n",
        "\n",
        "print(f\"# of samples: {len(X_full)}\")\n",
        "print(f\"# of clases: {len(class_names)}: {class_names}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luqIY__qTWi3",
        "outputId": "4289efaa-0fb7-475c-fc43-58b437183638"
      },
      "id": "luqIY__qTWi3",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets with labels may take a while\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n",
            "WARNING:google_auth_httplib2:httplib2 transport does not support per-request timeout. Set the timeout when constructing the httplib2.Http instance.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset successfully stored. ID in Drive: 1bMDRJiwA7kaN1U_2cOehiYZuhHrJ6Ofr\n",
            "# of samples: 20933\n",
            "# of clases: 43: ['abraham_grampa_simpson', 'agnes_skinner', 'apu_nahasapeemapetilon', 'barney_gumble', 'bart_simpson', 'carl_carlson', 'charles_montgomery_burns', 'chief_wiggum', 'cletus_spuckler', 'comic_book_guy', 'disco_stu', 'edna_krabappel', 'fat_tony', 'gil', 'groundskeeper_willie', 'homer_simpson', 'kent_brockman', 'krusty_the_clown', 'lenny_leonard', 'lionel_hutz', 'lisa_simpson', 'maggie_simpson', 'marge_simpson', 'martin_prince', 'mayor_quimby', 'milhouse_van_houten', 'miss_hoover', 'moe_szyslak', 'ned_flanders', 'nelson_muntz', 'otto_mann', 'patty_bouvier', 'principal_skinner', 'professor_john_frink', 'rainier_wolfcastle', 'ralph_wiggum', 'selma_bouvier', 'sideshow_bob', 'sideshow_mel', 'simpsons_dataset', 'snake_jailbird', 'troy_mcclure', 'waylon_smithers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Pre-processing"
      ],
      "metadata": {
        "id": "5iHCIgaBtcO3"
      },
      "id": "5iHCIgaBtcO3"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "import io\n",
        "import numpy as np\n",
        "\n",
        "def load_data_from_folder(folder_id, filename):\n",
        "    query = f\"name = '{filename}' and '{folder_id}' in parents and trashed = false\"\n",
        "    results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
        "    items = results.get('files', [])\n",
        "\n",
        "    if not items:\n",
        "        print(f\"No found the file '{filename}' in the folder '{folder_id}'.\")\n",
        "        return None, None, None\n",
        "\n",
        "    file_id = items[0]['id']\n",
        "    request = drive_service.files().get_media(fileId=file_id)\n",
        "    fh = io.BytesIO()\n",
        "    downloader = MediaIoBaseDownload(fh, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "        print(f\"Download progress: {int(status.progress() * 100)}%\")\n",
        "\n",
        "    # Load NumPy data from the memory buffer\n",
        "    fh.seek(0)\n",
        "    data = np.load(fh, allow_pickle=True)\n",
        "\n",
        "    X = data['X']\n",
        "    y = data['y']\n",
        "    labels = data['labels']\n",
        "\n",
        "    print(\"Data successfully loaded.\")\n",
        "    return X, y, labels\n",
        "\n",
        "\n",
        "X_full, y_full, class_names = load_data_from_folder(GOOGLE_DRIVE_FOLDER_ID, PRE_PROCESSED_DATASET)\n",
        "\n",
        "class_names"
      ],
      "metadata": {
        "id": "DTR0QyWptopC",
        "outputId": "f92d7fb5-3515-49a8-b125-e3d0de28aec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DTR0QyWptopC",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download progress: 69%\n",
            "Download progress: 100%\n",
            "Data successfully loaded.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['abraham_grampa_simpson', 'agnes_skinner',\n",
              "       'apu_nahasapeemapetilon', 'barney_gumble', 'bart_simpson',\n",
              "       'carl_carlson', 'charles_montgomery_burns', 'chief_wiggum',\n",
              "       'cletus_spuckler', 'comic_book_guy', 'disco_stu', 'edna_krabappel',\n",
              "       'fat_tony', 'gil', 'groundskeeper_willie', 'homer_simpson',\n",
              "       'kent_brockman', 'krusty_the_clown', 'lenny_leonard',\n",
              "       'lionel_hutz', 'lisa_simpson', 'maggie_simpson', 'marge_simpson',\n",
              "       'martin_prince', 'mayor_quimby', 'milhouse_van_houten',\n",
              "       'miss_hoover', 'moe_szyslak', 'ned_flanders', 'nelson_muntz',\n",
              "       'otto_mann', 'patty_bouvier', 'principal_skinner',\n",
              "       'professor_john_frink', 'rainier_wolfcastle', 'ralph_wiggum',\n",
              "       'selma_bouvier', 'sideshow_bob', 'sideshow_mel',\n",
              "       'simpsons_dataset', 'snake_jailbird', 'troy_mcclure',\n",
              "       'waylon_smithers'], dtype='<U24')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}